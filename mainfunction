! git clone https://github.com/marklok/context_data.git

!pip install llama-index==0.5.6
!pip install langchain==0.0.148

from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain import OpenAI
import sys
import os
from IPython.display import Markdown, display

def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 50
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600 

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name="text-davinci-003", max_tokens=num_outputs))
 
    documents = SimpleDirectoryReader(directory_path).load_data()
    
    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)

    index.save_to_disk('index.json')

    return index

def ask_ai():
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    while True: 
        query = input("What privacy questions do you have for me today? ")
        response = index.query(query)
        display(Markdown(f"Response: <b>{response.response}</b>"))
        
os.environ["OPENAI_API_KEY"] = input("Paste your OpenAI key here and hit enter:")

construct_index("context_data")

ask_ai()


def generate_questionnaire_response(user_input):
    # Define a list of predefined prompts and corresponding questions
    prompts = [
        ("What is your name?", "Please provide your full name."),
        ("What is your email address?", "Please provide your email address."),
        ("What is your date of birth?", "Please provide your date of birth."),
        # Add more prompts and questions as needed
    ]

    # Iterate through the predefined prompts and check if the user input matches any of them
    for prompt, question in prompts:
        if prompt.lower() in user_input.lower():
            return question

    # If no match is found, generate a generic follow-up question
    return "Could you please provide more information about that?"
